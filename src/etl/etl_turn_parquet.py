#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¶€ì²œì‹œ íšŒì „êµí†µëŸ‰ XML âœ Parquet (ì›ë³¸ í•„ë“œ 100 % ë³´ì¡´)
usage: python etl_turn_parquet.py
"""
from __future__ import annotations
import re
import glob
import json
from pathlib import Path

import xmltodict
import pandas as pd
import dask.dataframe as dd
from dask import delayed
from dask.diagnostics import ProgressBar
import dask

# ì›ë³¸ ë””ë ‰í† ë¦¬, ì¶œë ¥ ë””ë ‰í† ë¦¬
RAW_DIR = Path("0_raw/êµí†µëŸ‰ë°ì´í„°(íšŒì „êµí†µëŸ‰ê¸°ë°˜)")
OUT_DIR = Path("1_lake/turn.parquet")

# Mac ë¦¬ì†ŒìŠ¤ í¬í¬ë‚˜ ìˆ¨ê¹€ íŒŒì¼ í•„í„°ë§: ì‹¤ì œ xml íŒŒì¼ë§Œ
all_paths = glob.glob(str(RAW_DIR / "**" / "*.xml"), recursive=True)
xml_paths = [p for p in all_paths
             if not Path(p).name.startswith("._")
             and Path(p).suffix.lower() == ".xml"]
print(f"ğŸ” ì²˜ë¦¬ ëŒ€ìƒ XML íŒŒì¼ ìˆ˜: {len(xml_paths)}")

# ì°¨ëŸ‰ íƒ€ì… ì¶”ì¶œ ì •ê·œí‘œí˜„ì‹
VEH_RE = re.compile(r"([pbstlmk])(?:tra|total)?\.xml$", re.I)

# XML í•˜ë‚˜ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜

def parse_one_xml(path: str | Path) -> pd.DataFrame:
    path = Path(path)
    # íŒŒì¼ ë©”íƒ€
    route = path.parents[1].name  # ì˜ˆ: êµì°¨ë¡œ_26
    date = path.parents[0].name   # ì˜ˆ: 20220810
    m = VEH_RE.search(path.name)
    veh_type = m.group(1).lower() if m else "unknown"

    # XML íŒŒì‹±
    raw = path.read_text(encoding="utf-8")
    # resource forkë‚˜ ì œì–´ë¬¸ì ì œê±°
    raw = re.sub(r"[\x00-\x1F]+", "", raw)
    doc = xmltodict.parse(raw)

    intervals = doc.get("data", {}).get("interval", [])
    if isinstance(intervals, dict):
        intervals = [intervals]

    rows: list[dict] = []
    for itv in intervals:
        begin = itv.get("@begin")
        end = itv.get("@end")
        itv_id = itv.get("@id")
        edges = itv.get("edgeRelation", [])
        if isinstance(edges, dict):
            edges = [edges]
        for e in edges:
            rows.append({
                # ì›ë³¸ í•„ë“œ
                "begin": begin,
                "end": end,
                "interval_id": itv_id,
                "edge_from": e.get("@from"),
                "edge_to": e.get("@to"),
                "count": e.get("@count"),
                # ë©”íƒ€
                "route": route,
                "date": date,
                "veh_type": veh_type,
                "filepath": str(path),
            })

    df = pd.DataFrame(rows)

    # --- numeric type conversions for research-friendly schema ---
    df["begin"] = pd.to_numeric(df["begin"], errors="coerce").astype("float64")
    df["end"]   = pd.to_numeric(df["end"],   errors="coerce").astype("float64")
    df["date"]  = pd.to_numeric(df["date"],  errors="coerce").astype("Int32")

    # dtype ìºìŠ¤íŒ…
    str_cols = ["begin", "end", "interval_id",
                "edge_from", "edge_to",
                "route", "veh_type", "filepath"]
    for c in str_cols:
        df[c] = df[c].astype("string")
    df["count"] = pd.to_numeric(df["count"], errors="coerce").astype("Int32")
    # bucketed timestamp
    df["dt"] = (
        pd.to_datetime(df["begin"].astype("float64"), unit="s")
        .dt.floor("15min")
    )
    return df

# Dask ì„¤ì •: threads ìŠ¤ì¼€ì¤„ëŸ¬, ProgressBar
if __name__ == "__main__":
    dask.config.set(scheduler="threads")
    ProgressBar().register()

    # delayed list
    delayed_dfs = [delayed(parse_one_xml)(p) for p in xml_paths]

    # meta ì •ì˜
    meta = {
        "begin": "string",
        "end": "string",
        "interval_id": "string",
        "edge_from": "string",
        "edge_to": "string",
        "count": "Int32",
        "route": "string",
        "date": "Int32",
        "veh_type": "string",
        "filepath": "string",
        "dt": "datetime64[ns]",
    }
    ddf = dd.from_delayed(delayed_dfs, meta=meta)
    # repartition to a reasonable number for parallel writes
    ddf = ddf.repartition(npartitions=1000)

    # Parquet ì €ì¥
    print("â³ Parquet ì €ì¥ ì‹œì‘â€¦")
    ddf.to_parquet(
        OUT_DIR,
        engine="pyarrow",
        compression="zstd",
        partition_on=["route", "date", "veh_type"],
        write_index=False,
    )
    print(f"âœ“ ì™„ë£Œ â†’ {OUT_DIR}")

    # ì €ì¥ ì»¬ëŸ¼ í™•ì¸
    print("\nâ–  ì €ì¥ëœ ì»¬ëŸ¼:")
    print(json.dumps(list(ddf.columns), ensure_ascii=False, indent=2))