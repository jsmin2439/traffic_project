#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
etl_indicator_parquet.py

0_raw/ì§€í‘œë°ì´í„°/*.csv âœ 1_lake/indicator.parquet
  â€¢ ì›ë³¸ í•„ë“œ 100% ë³´ì¡´
  â€¢ Mac ë¦¬ì†ŒìŠ¤í¬í¬(. _*) ìë™ í•„í„°ë§
  â€¢ ì œì–´ë¬¸ì/ì“°ë ˆê¸°ê°’(<NA>/NaN) ìë™ ì œê±°
  â€¢ ì—°êµ¬ì— ë§ì¶˜ ì ì ˆí•œ dtype ìºìŠ¤íŒ…
usage: python etl_indicator_parquet.py
"""
from __future__ import annotations
import re, glob, json
from pathlib import Path

import pandas as pd
import dask.dataframe as dd
from dask import delayed
from dask.diagnostics import ProgressBar

RAW_DIR = "0_raw/ì§€í‘œë°ì´í„°"
OUT_DIR = "1_lake/indicator.parquet"

# â€” 1) ëª¨ë“  CSV ê²½ë¡œ ìˆ˜ì§‘ & Mac ë¦¬ì†ŒìŠ¤í¬í¬(. _*) í•„í„°ë§
all_csv = glob.glob(f"{RAW_DIR}/**/*.csv", recursive=True)
csv_paths = [
    p for p in all_csv
    if not Path(p).name.startswith("._")
       and p.lower().endswith("total.csv")
]
print(f"ğŸ” ì²˜ë¦¬ ëŒ€ìƒ CSV íŒŒì¼ ìˆ˜: {len(csv_paths):,}")

# íŒŒì¼ëª…ì—ì„œ ë‚ ì§œÂ·ì‹œê°„(dt) ì¶”ì¶œìš© ì •ê·œì‹
FNAME_RE = re.compile(r"(\d{8})(\d{6})total\.csv$", re.IGNORECASE)

def _clean_str_series(s: pd.Series) -> pd.Series:
    """string dtypeì—ì„œ ì œì–´ë¬¸ì ì œê±°, invalidëŠ” <NA> ì²˜ë¦¬"""
    return (
        s
        .astype("string")
        .str.replace(r"[\x00-\x1F]", "", regex=True)
        .replace("", pd.NA)
    )

# â€” 2) í•˜ë‚˜ì˜ CSV â†’ pandas.DataFrame
def parse_one_csv(path: str | Path) -> pd.DataFrame:
    path = Path(path)
    route = path.parents[1].name       # ex) êµì°¨ë¡œ_26
    date_str = path.parents[0].name    # ex) 20220810

    # íŒŒì¼ëª…ì—ì„œ dt ìƒì„±
    m = FNAME_RE.search(path.name)
    if not m:
        raise RuntimeError(f"unexpected filename: {path.name}")
    dt = pd.to_datetime(m.group(1) + m.group(2),
                        format="%Y%m%d%H%M%S")

    # ---- ì›ë³¸ ì½ê¸° ----
    df = pd.read_csv(
        path,
        dtype={
            "edge_grp_id": "string",
            "avg_spd":       "string",
            "total_time_loss": "string",
            "avg_time_loss":   "string",
            "avg_travle_time": "string",
        },
        na_values=["", "NA", "NaN"],
        keep_default_na=True
    )

    # ---- ì œì–´ë¬¸ì ì œê±° & íƒ€ì… ë³€í™˜ ----
    df["edge_grp_id"]      = _clean_str_series(df["edge_grp_id"])
    df["avg_spd"]          = pd.to_numeric(df["avg_spd"], errors="coerce")
    df["total_time_loss"]  = pd.to_numeric(df["total_time_loss"], errors="coerce")
    df["avg_time_loss"]    = pd.to_numeric(df["avg_time_loss"], errors="coerce")
    df["avg_travle_time"]  = pd.to_numeric(df["avg_travle_time"], errors="coerce")

    # ---- ë©”íƒ€ ì»¬ëŸ¼ ì¶”ê°€ ----
    df["route"]    = route
    df["date"]     = int(date_str)
    df["dt"]       = dt
    df["veh_type"] = "X"   # ì§€í‘œë°ì´í„°ëŠ” ê³ ì •

    # pandas dtypes í†µì¼
    df["route"]    = df["route"].astype("string")
    df["veh_type"] = df["veh_type"].astype("category")
    df["date"]     = df["date"].astype("int32")

    # ì»¬ëŸ¼ ìˆœì„œ ë³´ì¡´
    cols = [
      "edge_grp_id",
      "avg_spd", "total_time_loss", "avg_time_loss", "avg_travle_time",
      "route", "date", "dt", "veh_type"
    ]
    return df[cols]

# â€” 3) Daskë¡œ ë³‘ë ¬ ì²˜ë¦¬ & Parquet ì €ì¥ ----
delayed_dfs = [delayed(parse_one_csv)(p) for p in csv_paths]
meta = {
    "edge_grp_id":      "string",
    "avg_spd":          "float64",
    "total_time_loss":  "float64",
    "avg_time_loss":    "float64",
    "avg_travle_time":  "float64",
    "route":            "string",
    "date":             "int32",
    "dt":               "datetime64[ns]",
    "veh_type":         "category",
}
ddf = dd.from_delayed(delayed_dfs, meta=meta)

ProgressBar().register()
ddf.to_parquet(
    OUT_DIR,
    engine="pyarrow",
    compression="zstd",
    partition_on=["route", "date"],
    write_index=False,
)

print(f"\nâœ… Parquet ì €ì¥ ì™„ë£Œ â†’ {OUT_DIR}\n")
print("â–  ì €ì¥ë˜ëŠ” ì»¬ëŸ¼:")
print(json.dumps(list(ddf.columns), ensure_ascii=False, indent=2))