#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
etl_od_parquet.py

ë¶€ì²œì‹œ ê¸°ì¢…ì  êµí†µëŸ‰ CSV â†’ Parquet
 Mac ë¦¬ì†ŒìŠ¤í¬í¬ ìë™ í•„í„°ë§
 ì œì–´ë¬¸ì ì œê±°
 ì›ë³¸ ì¹¼ëŸ¼ 100% ë³´ì¡´ + ë©”íƒ€ì¹¼ëŸ¼ ì¶”ê°€
 ì ì ˆí•œ dtype ìºìŠ¤íŒ…
usage: python etl_od_parquet.py
"""
from __future__ import annotations
import re, glob, json
from pathlib import Path

import pandas as pd
import dask.dataframe as dd
from dask import delayed
from dask.diagnostics import ProgressBar

RAW_DIR   = "0_raw/êµí†µëŸ‰ë°ì´í„°(ê¸°ì¢…ì êµí†µëŸ‰ê¸°ë°˜)"
OUT_DIR   = "1_lake/od.parquet"
GLOB_PAT  = f"{RAW_DIR}/**/*.csv"

# ì œì–´ë¬¸ì/ë„ ì œê±°ìš© ì •ê·œì‹
PAT_CTRL = re.compile(r"[\x00-\x1F]+")

# 1) íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ & ë¦¬ì†ŒìŠ¤í¬í¬(. _*) í•„í„°ë§
all_csvs = glob.glob(GLOB_PAT, recursive=True)
csv_paths = [
    p for p in all_csvs
    if not Path(p).name.startswith("._")
       and p.lower().endswith(".csv")
]
print(f"ğŸ” ì²˜ë¦¬ ëŒ€ìƒ CSV íŒŒì¼ ìˆ˜: {len(csv_paths):,}")

# 2) ë‹¨ì¼ CSV â†’ pandas.DataFrame
def parse_one_csv(path: str) -> pd.DataFrame:
    path = Path(path)
    # â”€â”€ ë©”íƒ€ ì •ë³´ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    route = path.parents[1].name      # ex) êµì°¨ë¡œ_26
    date  = path.parents[0].name      # ex) 20220810
    # â”€â”€ CSV ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df = pd.read_csv(path, dtype=str)  # ì¼ë‹¨ ì „ë¶€ ë¬¸ìì—´ë¡œ
    # â”€â”€ ì œì–´ë¬¸ì ì œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for col in df.columns:
        # NaNë„ ë¬¸ìì—´ë¡œ ì½íˆë¯€ë¡œ str â†’ ì œê±° â†’ ë‹¤ì‹œ NA ì²˜ë¦¬
        df[col] = (
            df[col]
            .fillna("")                      # NA â†’ ""
            .astype(str)
            .str.replace(PAT_CTRL, "", regex=True)
            .replace({"": pd.NA})           # "" â†’ NA
        )
    # â”€â”€ ë©”íƒ€ì¹¼ëŸ¼ ì¶”ê°€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df["route"] = route
    df["date"]  = date
    df["path"]  = str(path)
    # â”€â”€ ìë£Œí˜• ìºìŠ¤íŒ… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œ ì¹¼ëŸ¼ë“¤ë§Œ ë³€í™˜
    to_int = [
        "unix_time","stdr_ymd","stdr_hm","stdr_ss",
        "vhcl_typ","que_all","que_200","que_200_500","que_500"
    ]
    to_float = [
        "lon","lat","alt","spd","allowed_spd",
        "agl","slp","dist2to_inter","dist2nxt_inter","tl"
    ]
    for c in to_int:
        if c in df:
            df[c] = pd.to_numeric(df[c], errors="coerce").astype("Int64")
    for c in to_float:
        if c in df:
            df[c] = pd.to_numeric(df[c], errors="coerce").astype("Float64")
    # (ë‚˜ë¨¸ì§€ ì¹¼ëŸ¼ì€ pandas.StringDtype)
    str_cols = [c for c in df.columns if df[c].dtype == object]
    df[str_cols] = df[str_cols].astype("string")
    return df

# 3) Daskë¡œ ë³‘í•© & dt ìƒì„±
delayed_dfs = [delayed(parse_one_csv)(p) for p in csv_paths]
# ë©”íƒ€ ì •ì˜
meta = parse_one_csv(csv_paths[0]).iloc[0:0]
ddf = dd.from_delayed(delayed_dfs, meta=meta)

# dt: unix_time â†’ datetime â†’ 15ë¶„ ë²„í‚·
ddf["dt"] = (
    dd.to_datetime(ddf["unix_time"].astype("int64"), unit="s", errors="coerce")
      .dt.floor("15min")
)

# 4) Parquet ì €ì¥
print("â³ Parquet ì €ì¥ ì‹œì‘â€¦")
ProgressBar().register()
ddf.to_parquet(
    OUT_DIR,
    engine="pyarrow",
    compression="zstd",
    partition_on=["route", "date"],
    write_index=False,
)
print(f"âœ“ ì €ì¥ ì™„ë£Œ â†’ {OUT_DIR}")

# 5) ìµœì¢… ì¹¼ëŸ¼ ëª©ë¡ ì¶œë ¥
cols = list(ddf.columns)
print("\nâ–  ì €ì¥ë˜ëŠ” ì¹¼ëŸ¼:")
print(json.dumps(cols, ensure_ascii=False, indent=2))